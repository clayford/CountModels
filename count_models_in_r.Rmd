---
title: "Modeling Count Data with R"
author: "Clay Ford, Statistical Research Consultant, UVA Library"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

## Quick Intro to R Notebooks and R Markdown

This is an R Markdown Notebook. When you execute R code within the notebook, the results appear beneath the code.  

This file was created in RStudio by going to File...New File...R Notebook.

R code needs to be in "chunks" in an R Markdown Notebook. Below is an example of an R code chunk. It makes a parabola.

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter* (Win/Linux) or *Cmd+Shift+Return* (Mac). 

```{r}
x <- seq(-1, 1, by = 0.01)
y <- x^2
plot(x, y, type = "l")
```

To hide the output, click the Expand/Collapse output button. To clear results (or an error), click the "x". 

You can also press *Ctrl+Enter* (Win/Linux) or *Cmd+Return* (Mac) to run one line of code at a time (instead of the entire chunk).

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).  

## CODE ALONG 0

Insert a new R code chunk below and type and run the code: Sys.time()


## Agenda

- linear model review
- explore distributions for counts: Poisson and Negative Binomial
- fitting/interpreting Poisson and Negative Binomial count models
- evaluate count models with rootograms
- fitting zero-inflated models
- using effect plots to visualize models

Appendix Topics (topics cut for time)
- rate models
- hurdle models
- creating rootograms "by hand" 
- glm residuals 
- fitting quasipoisson models
- simulate zero-inflated negative binomial data

## Packages

Let's load some packages we're going to use today.

```{r}
# install.packages("countreg", repos="http://R-Forge.R-project.org")
library(countreg)

library(pscl)
library(AER)
library(ggplot2)
library(ggeffects)
```

## Linear Model Review

Below I generate two variables: `x` and `y`. Notice `y` depends on `x`. We might say `x` is the _independent_ variable and `y` is the _dependent_ variable. 

```{r}
x <- 1:25
y <- 10 + 5*x
plot(x, y)
```


10 is the intercept, 5 is the slope.
y is completely determined by x.

Let's add some "noise" to our data by adding random draws from a Normal distribution with mean = 0 and a standard deviation = 10.

`set.seed(1)` ensures we all get the same "random" data

```{r}
set.seed(1)
noise <- rnorm(n = 25, mean = 0, sd = 10)
```


Add the noise to 10 + 5*x and re-draw plot

```{r}
y <- 10 + 5*x + noise
plot(x, y)
```

Now `y` is associated with `x` but not completely determined by `x`.

This data is the combination of two parts:

1. _fixed_: 10 + 5*x
2. _random_: rnorm(n = 25, mean = 0, sd = 10)

We could also combine the two parts in a single call to `rnorm()` where we enter `10 + 5*x` as the mean. In other words, randomly generate Normal data with a _conditional mean_.

```{r}
set.seed(1)
y <- rnorm(n = 25, mean = 10 + 5*x, sd = 10)
plot(x, y)
```


Each y is drawn from a Normal distribution with mean _dependent on x_.

What if we were given this data and told to determine the process that
generated it? In other words, work backwards and fill in the blanks:

1. ____ + ____*x  (fixed part, the model)
2. rnorm(n = 25, mean = 0, sd = ____)  (random part, error)

That's basically what linear modeling is. Proposing a model and finding the most likely values of the assumed probability distribution that generated the data.

Traditional linear modeling assumes the following:

1. the formula is a weighted sum of predictors (eg, 10 + 5x)
2. the noise is a random draw from a Normal distribution with mean = 0
3. the standard deviation of the Normal distribution is constant

Linear modeling tries to recover the weights (or coefficients) in the first assumption (10, 5) and the standard deviation in the 3rd assumption (10).

To do this we can use `lm()`. The syntax "y ~ x" means we think the model is "y = intercept + slope*x". We usually save the model into an object. Below I save it to "mod".

```{r}
mod <- lm(y ~ x)
summary(mod)
```

The model returns the following estimates:

- rnorm(mean = 11.135 + 5.042*x, sd = 9.7)

This is close to the "true" values we used to generate the data:

- rnorm(mean = 10 + 5*x, sd = 10)

This is because we fit the correct model. We simulated the data, so we knew which model to fit. This doesn't happen in real life.

We might use our model to make a prediction. For example, expected value of y when x = 10 with a 95% confidence interval.

```{r}
predict(mod, data.frame(x = 10), interval = "confidence")
```

We could also plot our fitted line through the data since we only have two dimensions. The symbol `|>` is a "pipe" that takes the result of the function on the left and "pipes" it into the first argument of the function on the right. We use the `ggpredict()` function from the ggeffects package to generate the predicted values and then pipe the values into a `plot()` method provided by ggeffects.

```{r}
ggpredict(mod, terms = "x") |> plot(add.data = TRUE)

# equivalent to...
# plot(ggpredict(mod, terms = "x"), add.data = TRUE)
```


Today we look at _count models_ where we model integers greater than or equal to 0 using different distributions for the random error (ie, not a Normal distribution).

## Distributions for counts

Let's review two common distributions for count data. Count data are integers greater than or equal to 0. These are discrete values we might use to record counts of events.

### Poisson distribution

The Poisson distribution is a discrete probability distribution with one parameter: _lambda_.

lambda is the mean and the variance; the _variance and mean are equal_ in Poisson distributions.

Let's explore the Poisson distribution via simulation. We can use the `rpois()` function to generate random data with a specified lambda (mean). Notice the data are positive integers greater than or equal to 0.

Sample 1000 observations from a Poisson distribution with lambda = 3. Note that lambda can take decimal values.

```{r}
set.seed(1)
y1 <- rpois(n = 1000, lambda = 3)
head(y1)
```

Create table of the counts of distinct values. Top row is the count, bottom row is the number of occurrences.

```{r}
table(y1)
```


Visualize distribution of counts

```{r}
table(y1) |> plot()
```


We can also visualize the proportion of counts.

```{r}
table(y1) |> proportions() |> plot()
```

Notice the mean and variance of the simulated data are roughly equal:

```{r}
c(mean(y1), var(y1)) 
```



### Negative binomial distribution

The negative binomial distribution is also a discrete probability distribution, but with two parameters: _mu_ (mean) and _theta_ (a dispersion parameter)

mu is the mean. Theta is a dispersion parameter that allows the _variance to be greater than the mean_.

Variance = mu + mu^2/theta

This can be useful when the assumption of count data having equal mean and variance is too restrictive or unrealistic.

Let's explore the negative binomial distribution via simulation. We can use the `rnbinom()` function to generate random data with a specified mean (`mu`) and dispersion parameter (`size`).

Let's sample 1000 observations from a negative binomial distribution with a mean = 3 and size (theta) = 2. Notice the data are positive integers greater than or equal to 0.

```{r}
set.seed(2)
y2 <- rnbinom(n = 1000, mu = 3, size = 2)
head(y2)
```

Create table of the counts of distinct values. Top row is the count, bottom row is the number of occurrences.

```{r}
table(y2)
```

Visualize the distribution of counts

```{r}
table(y2) |> plot()
```

Visualize the proportion of counts:

```{r}
table(y2) |> proportions() |> plot()
```

The mean and variance are not equal. The variance is larger.

```{r}
c(mean(y2), var(y2)) 
```

As theta gets big, the negative binomial converges to a Poisson distribution. Notice in the randomly generated data below with a huge theta (2000) that the estimated mean and variance are about equal, like a Poisson distribution.

```{r}
y3 <- rnbinom(n = 1000, mu = 3, size = 2000)
c(mean(y3), var(y3)) 
```


## Simulating count data with conditional mean

Above we simulated count data using a single mean. But perhaps the mean depends on some other variable(s). Perhaps the mean is lower for an "untreated" group than the mean for a "treated" group. This is a simple example of statistical modeling, which estimates the mean of a dependent variable _conditional_ on values of predictor variables. 

Let's simulate 1000 0s and 1s to indicate untreated (0) and treated (1)

```{r}
set.seed(11)
trt <- sample(0:1, size = 1000, replace = T)
```

Now we randomly sample from a Poisson distribution with a mean of `exp(1.2 + 1.8*trt)`. The `exp()` function means exponentiate, or raise e to the power of the argument. Why use `exp()`? That ensures we get a positive value. Lambda has to be positive. Here it's not necessary but as we'll see this transformation is built into count models.

```{r}
set.seed(22)
y_resp <- rpois(n = 1000, lambda = exp(1.2 + 1.8*trt))
```

Expected count when trt = 0

```{r}
exp(1.2) # about 3
```

expected count when trt = 1

```{r}
exp(1.2 + 1.8) # about 20
```

Plot of y1 distribution. Notice the two distinct humps for each group.

```{r}
table(y_resp) |> plot()
```

Let's do the same for a negative binomial distribution

Randomly sample from a negative binomial distribution with a mean of `exp(1.2 + 1.8*trt)` and a dispersion of 10. Notice again the two distinct humps and that the data is much more dispersed (ie, has more variance)

```{r}
set.seed(33)
y2_resp <- rnbinom(n = 1000, mu = exp(1.2 + 1.8*trt), size = 10)
table(y2_resp) |> plot()
```

## Count modeling with simulated data

Let's pretend we don't know the coefficients in the formula used to generate the counts: 

___ + ___*trt. 

How to recover those "true" values? This is essentially count modeling.

### Fitting a Poisson count model

We use the `glm()` function to fit a poisson count model. Use the same way as `lm()`, but include the family argument: `family = poisson` 

The `link = "log"` argument is assumed by default but is included here for completeness. Why "log"? Because it's the _link_ to the raw additive model. Recall that taking the log of an exponentiated value returns the original value.

```{r}
log(exp(3))
```

Likewise...

```
log(exp(1.2 + 1.8*trt)) = 1.2 + 1.8*trt
```

Let's fit a model to our simulated data. We generated the data so we know the correct model to specify!

The "correct" model is `y_resp = (Intercept) + b1*trt` because we simulated the data using the following code where 1.2 is the intercept and 1.8 is b1, short for beta_1, which is how model coefficients are often symbolized.

`y_resp <- rpois(n = 1000, lambda = exp(1.2 + 1.8*trt))`

Recall:
`y_resp ~ trt` is shorthand for "y_resp = (Intercept) + b1*trt"

```{r}
m1 <- glm(y_resp ~ trt, family = poisson(link = "log"))
summary(m1)
```

We can extract the coefficients using `coef()`

```{r}
coef(m1)
```

The coefficients are very close to the "true" values of 1.2 and 1.8.

Notice the coefficients are on the log scale. We need to use `exp()` to get expected counts. 

When trt == 0 the expected mean count is...

exp(1.176377 + 1.814068 * 0) = exp(1.176377) = about 3

```{r}
coef(m1)["(Intercept)"] |> exp()
```

When trt == 1 (sum both coefficients) the expected mean is...

exp(1.176377 + 1.814068 * 1) = exp(2.990444) =  about 20

```{r}
sum(coef(m1)) |> exp()
```

Exponentiating the slope coefficient tells us the _multiplicative effect_ of trt == 1. Expected count of y when trt == 1 is about 6.1 times the expected count when trt == 0.

```{r}
coef(m1)["trt"] |> exp()
```

Multiplying the exponentiated intercept (ie, the expected count when trt == 0) by 6.135355 returns the expected count for trt == 1

```{r}
exp(coef(m1)["(Intercept)"]) * 6.135355 
```

This is precisely the expected count when trt == 1 that we calculated above.

```{r}
sum(coef(m1)) |> exp()
```

Let's use our model to simulate some data and see if it looks similar to our observed data set. Below we use `rpois()` again but this time with our model estimated coefficients. We then plot the original distribution of the raw data and compare it to the distribution of the model-simulated data. (Feel free to run this code multiple times to see how the model-simulated data changes)

```{r}
sim_y_resp <- rpois(n = 1000, lambda = exp(1.176377 + 1.814068*trt))
par(mfrow = c(1,2))
plot(table(y_resp), main = "original")
plot(table(sim_y_resp), main = "model-simulated")
```

Instead of manually using the `rpois()` function we could use the `simulate()` function with the fitted model object as follows. `nsim = 1` says to generate one set of data from the model.

```{r}
sim_y_resp <- simulate(m1, nsim = 1)
par(mfrow = c(1,2))
plot(table(y_resp), main = "original")
plot(table(sim_y_resp), main = "model-simulated")
```


### Fitting a Negative binomial count model

We use the `glm.nb()` function from the MASS package to fit a negative binomial model. No need to specify the family argument. The `link = "log"` argument is assumed by default but is included here for completeness.

Let's fit a model to our simulated data. We generated the data so we know the correct model to specify!

`y2_resp <- rnbinom(n = 1000, mu = exp(1.2 + 1.8*trt), size = 10)`

In addition to the coefficients for the additive model, `glm.nb()` will estimate theta.

```{r}
m2 <- glm.nb(y2_resp ~ trt, link = "log")
summary(m2)
```

Notice the phrase: "(Dispersion parameter for Negative Binomial(9.8611) family taken to be 1)"

This is confusing because "Dispersion parameter" in this sentence refers to the GLM dispersion parameter (phi), which is different from the negative binomial dispersion parameter (theta)! The GLM dispersion parameter gets estimated when we fit models with `family = quasipoisson(link = "log")`. (See Appendix: Fitting quasipoisson models)

We can extract theta from the model object if needed:

```{r}
m2$theta
```

And we can use `coef()` to extract the coefficients.

```{r}
coef(m2)
```

Again, the coefficients are very close to the "true" values of 1.2, 1.8 and 10.

And again we need to use `exp()` to get expected counts.

When trt == 0 the expected mean count is about 3

```{r}
coef(m2)["(Intercept)"] |> exp()
```

When trt == 1 (sum both coefficients) the expected mean count is about 20

```{r}
sum(coef(m2)) |> exp()
```

The multiplicative effect of trt==1 is about 5.9.

```{r}
coef(m2)["trt"] |> exp()
```

Let's use our model to simulate some data and see if it looks similar to our observed data set. Once again we can use the `simulate()` function to do this.


```{r}
sim_y2_resp <- simulate(m2, nsim = 1)
par(mfrow = c(1,2))
plot(table(y2_resp), main = "original")
plot(table(sim_y2_resp), main = "model-simulated")

```

These look very similar because we fit the correct model. 

## CODE ALONG 1

Run the following lines of code to generate y which is a count conditional on variables "grp" and "x1". "grp" is a binary indicator that takes two values: "a" and "b". "x1" is a numeric value ranging uniformly from 1 to 10.

```{r}
set.seed(4)
grp <- sample(c("a","b"), size = 300, replace = T)
x1 <- runif(n = 300, min = 1, max = 10)
y <- rpois(n = 300, lambda = exp(1.8 + 1.5*(grp=="b") + -0.6*x1))
```

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).  

1. Fit a model that attempts to "recover" the true values in the model specified above. Call the model 'mod2'



2. Simulate data from the model and compare to the original observed data.



## Model Summary and Deviance

Let's look again at the summary of m1, the poisson model:

```{r}
summary(m1)
```

The _Deviance Residuals_ section summarizes the distribution of the residuals. Residuals represent the difference between what we observed and what our model predicts. When the fitted model is correct, deviance residuals should have an approximate standard normal distribution N(0,1). This provides a quick check of symmetry and range of the residuals.

```
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.0353  -0.7432  -0.1365   0.4641   3.3703
```

The _Coefficients_ section presents null hypothesis tests that the predictor coefficients are 0. The `z value` is the test statistic, which is equal to the coefficient estimate divided by the standard error. The p-value, `Pr(>|z|)` is the probability of seeing a coefficient as big or bigger (or as small or smaller) if the coefficient really was 0.

```
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  1.17638    0.02466   47.70   <2e-16 ***
trt          1.81407    0.02665   68.07   <2e-16 ***
```

The _deviance_ section presents two sums of squared residuals: Null deviance and Residual deviance. 

```
    Null deviance: 7784.1  on 999  degrees of freedom
Residual deviance: 1099.3  on 998  degrees of freedom

```

Residuals represent the difference between what we observed and what our model predicts. There are several types of residuals for GLM models. The Null and Residual deviance are the sum of squares of the _deviance residuals_.

Residual deviance "by hand"

```{r}
sum(residuals(m1, type = "deviance")^2)
```


Null deviance "by hand"

The null deviance is the sum of squared deviance residuals for _a model with no predictors_, just an intercept: y1 ~ 1

```{r}
null_m1 <- glm(y1 ~ 1, family = poisson)
sum(residuals(null_m1, type = "deviance")^2)

```

Three things to remember about deviance...

1. Deviance is a measure of error; we would like the Residual deviance to be much lower than the Null deviance.
2. If a predictor is just noise and has no explanatory power, we expect deviance to decrease by 1 on average.
3. When a useful predictor is added to a model, we expect deviance to decrease by more than 1.

We see a huge decrease in deviance in m1. 


## Poisson count modeling with realistic data

Let's load some realistic data to explore and model. A data set giving the number of publications (articles) by doctoral candidates in biochemistry in relation to various predictors. 

Source: Long, J. S. (1997). Regression Models for Categorical and Limited Dependent Variables, Sage.

Of interest is if the count of published articles can be modeled as a function of the other variables. We don't know the "true" model or whether or not these variables have anything to do with the count of articles.

```{r}
data("PhDPublications", package = "AER")
# create dataset with smaller name
d <- PhDPublications
rm(PhDPublications)
```

- articles: number of articles published in final three years of PhD studies
- gender: factor indicating gender.
- married: factor. Is the PhD student married?
- kids: Number of children less than 6 years old
- prestige: prestige score of the PhD department; higher the number the more prestigious the program.
- mentor: number of publications by the mentor in the preceeding three years

```{r}
summary(d)
```

Plot the response variable: number of published articles

```{r}
plot(table(d$articles))
```

The number of published articles has quite a bit of variability. Do the other variables perhaps "explain" some of the variability?

Let's fit a Poisson model using just gender and mentor as predictors. 

```{r}
phd.pois <- glm(articles ~ gender + mentor, 
                data = d, 
                family = poisson)
summary(phd.pois)
```

The Residual deviance is much lower than the Null deviance, suggesting the female and mentor variables help explain some of the variability in article count and are at least better than simply taking the mean (ie, the null intercept-only model).

Naive interpretation of coefficients.

The coefficients represent the increase/decrease in the _log expected count_ of articles. 

```{r}
coef(phd.pois)
```


If we exponentiate, we get the multiplicative effect (holding other predictors constant.)

```{r}
coef(phd.pois) |> exp()

```

The expected number of publications for female candidates is about 0.83 times that of male candidates, or about a 17% decrease

Each additional mentor publication increases expected count of published articles by a factor of about 1.025, or about 2.5%.

Once we exponentiate, we can subtract 1 to get the estimated proportional increase or decrease in the count.

```{r}
exp(coef(phd.pois)) - 1
```

All together in a matrix using cbind(), rounded to 3 decimal places

```{r}
round(
  cbind(beta = coef(phd.pois),
        exp_beta = exp(coef(phd.pois)),
        prop = exp(coef(phd.pois)) - 1),
  3)

```

Notice the proportional increase/decrease is very similar to the raw coefficients on the log scale. 

### Confidence intervals

Confidence intervals give us a sense of how certain these estimated effects are. Since we exponentiate and get a multiplicative effect, overlapping 1 (instead of 0) indicates an uncertain direction in magnitude.

```{r}
exp(confint(phd.pois))
```


The effect of female is estimated to decrease expected published article count anywhere from a factor of 0.75 to 0.92, or 25% to 8%.

_We should check the fit of the model before putting too much stock into these interpretations_. We could do something like the following, where we simulate counts from our model and compare to the observed data. Notice our model is estimating much fewer 0 counts and far fewer bigger counts. Re-run the chunk multiple times if you like.

```{r}
sim_articles <- simulate(phd.pois)
par(mfrow = c(1,2))
plot(table(d$articles))
plot(table(sim_articles$sim_1))
```

This doesn't look good. We'll explore another model fit visualization below called _rootograms_.

## CODE ALONG 2

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac). 

1. Add kids to the previous model and save the result as `phd.pois2`:

```
phd.pois <- glm(articles ~ gender + mentor, 
                data = d, family = poisson)
```



2. Interpret the coefficient for kids




## Evaluating model fit with rootograms 

A good fitting model should generate counts that look similar to the original data.

In the previous code along exercise we proposed the following model:

`articles = (Intercept) + gender + mentor + kids`

The result of the model was an intercept and coefficients for the 3 predictors

```{r}
coef(phd.pois2)

```


Previously we simulated data and compared bar plots of the original data with the simulated data. Another way to visualize model fit for count models is through _rootograms_.

The countreg package provides the `rootogram()` function to quickly generate "hanging" rootograms to visualize goodness of fit.

```{r}
countreg::rootogram(phd.pois2)
```

The red dots and connector lines visualize the model's expected number of articles.

The length of the bars are the _observed counts_. Notice the counts have been transformed with a square root transformation. That helps smaller numbers of counts not get squashed by larger numbers of counts.

Bars that dip way below 0 are _underfit_; bars that hang way above 0 are _overfit_.

We can use the max argument to increase or decrease the count displayed

```{r}
countreg::rootogram(phd.pois, max = 15)
```


There are two other styles available: standing and suspended

The standing version plots the model predicted counts (red line) over the observed counts (bars).

```{r}
countreg::rootogram(phd.pois2, style = "standing")
```

The suspended version uses less ink and shows just the under and over-fit counts.

```{r}
countreg::rootogram(phd.pois2, style = "suspended")
```

If preferred, we can request a ggplot2 version

```{r}
autoplot(rootogram(phd.pois2))
```

This is an ok not great fitting model. We are underfitting 0 counts, overfitting 1, 2, and 3 counts, then underfitting counts 4 - 7.

Recall our earlier toy model for which we fit the "correct" model. Notice we're not consistently over or underfitting counts. The counts seem to vary around 0 randomly.

```{r}
countreg::rootogram(m1)
```

See Appendix at conclusion of notebook for how to make rootograms "by hand".

## CODE ALONG 3

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac). 

1. Add married and prestige to the following model and save as a new model object named "phd.pois3".

```
phd.pois <- glm(articles ~ gender + mentor + kids, data = d, family = poisson)
```



2. Create a rootogram of the model



## Negative Binomial count modeling with real data

Recall the negative binomial model accommodates unequal mean and variance. This allows us to model "overdispersion". That is, when we have counts with larger variance than their mean.

The PhdPubs data appears to exhibit some overdispersion

```{r}
c(mean(d$articles), 
  var(d$articles))
  
```


The AER package provides a formal test of overdispersion with the `dispersiontest()` function. The null is the mean and variance are equal. Rejecting the null with a small p-value provides evidence of overdispersion.

Let's fit a new Poisson model with all predictors.

```{r}
phd.pois3 <- glm(articles ~ gender + married + kids + 
                   married + prestige + mentor,
                 data = d, family = poisson)
```

Now test for overdispersion using the dispersiontest() function.

```{r}
AER::dispersiontest(phd.pois3)
```

The super small p-value confirms our suspicion that we probably have overdispersion and that a Poisson model may not be suitable, which is also what we saw with the rootogram. In this case a negative binomial model may be more suitable since it accommodates overdispersion.
 
Recall that the `glm.nb()` function in the MASS package fits a negative binomial model. The syntax `articles ~ .` models articles as a function of all other columns in the PhdPubs data frame.

```{r}
phd.nb <- glm.nb(articles ~ ., data = d)
summary(phd.nb)
```

The interpretation of the coefficients is the same as a Poisson model.

```{r}
exp(coef(phd.nb))
```
 
Is this model better than the Poisson model? We can asses this question using the AIC/BIC information criteria. Both provide scores that estimate how models will perform with out-of-sample data. Models with lower values are preferred.

```{r}
AIC(phd.pois3, phd.nb)
```

```{r}
BIC(phd.pois3, phd.nb)
```

df is the number of model parameters. The negative binomial model has one extra parameter: the dispersion parameter. It certainly seems like the negative binomial model is better than a Poisson model.

A rootogram allows us to asses how well the model fits the observed data.

```{r}
countreg::rootogram(phd.nb)
```

This certainly seems to fit much better.

## Zero-inflated count models

It's not unusual for real life count data to have more zeroes than would be expected of a Poisson or negative binomial model. When this happens we say we have "zero inflation".

A common approach to modeling such data is to create a _mixture_ of two models to account for the _extra zeroes_:

1. a model for the occurrence of zeroes
2. a model for the counts, which can also include zeroes

Example: survey random sample of UVA students and ask how many alcoholic drinks they consumed over the most recent weekend.

Two sources of zeroes:
- Some don't drink, always zero
- Some do drink but perhaps didn't drink that weekend, second source of zeroes

Let's simulate some toy data with these qualities.

To begin we'll simulate zeroes and ones using `rbinom()` which allows us to randomly sample from a binomial distribution. A binomial distribution has two parameters: 

- size (number of "trials") 
- probability (chance of "success"). 

Below we generate 300 draws from a binomial distribution with size = 1 and prob = 0.7. This says 70% of the time we will draw a 1, which implies 30% of the time we will draw 0.

```{r}
set.seed(10)
zi <- rbinom(300, size = 1, prob = 0.7)
```

Next we'll create a simple binary predictor variable called "trt".

```{r}
trt <- rep(0:1, each = 150)
```

Now we'll simulate counts _conditional_ on our draws from the binomial distribution. If it's a 0, keep it. That's our count. These are the "always zero" values. Otherwise draw a value from a Poisson distribution with mean = exp(0.05 + 0.8*trt). This will produce additional zeroes.

The mean of the Poisson distribution is as follows: 

1. exp(0.05) when trt = 0
2. exp(0.05 + 0.8) = exp(0.85) when trt = 1

```{r}
set.seed(11)
y <- ifelse(zi == 0, 0, rpois(300, lambda=exp(0.05 + 0.8*trt)))  
dat <- data.frame(y, trt)
plot(table(dat$y))
```

Now let's fit a model to our data that does _not_ accommodate excess zeroes and look at the rootogram

```{r}
mod1 <- glm(y ~ trt, data = dat, family = poisson)
summary(mod1)
```

The summary looks good if you're into significant p-values, but the rootogram tells a different story.

```{r}
countreg::rootogram(mod1)
```

Underfitting zeroes and overfitting counts of 1 and 2 is characteristic of zero-inflated data.
 
Now let's fit the "correct" model by accommodating the excess zeroes. For this we'll use the `zeroinfl()` function from the pscl package. This fits two models, separated by a `|`:

1. a count model (counts that include zeroes)
2. a binomial model (only zeroes)

Notice we have to specify the distribution for the count model: `dist = "poisson"`. The count model is `y ~ trt`. The binomial model is `y ~ 1`, which is an intercept-only model (no predictors). This is the correct model because that's how we generated the data!

Notice the summary shows results for two models: a count model and a zero-inflation model.

```{r}
mod2 <- pscl::zeroinfl(y ~ trt | 1, data = dat, dist = "poisson")
summary(mod2)
```

The rootogram shows a much better fitting model.

```{r}
countreg::rootogram(mod2)
```


We can use the `plogis()` function to take the inverse logit of the binomial model coefficient to get the estimated probability of being an "always zero" count. Notice we come close to the "true" value of 0.3 we used to simulate the only-zeroes data.
 
```{r}
plogis(coef(mod2)["zero_(Intercept)"])
```

If we don't specify anything after the |, the `zeroinfl()` function assumes you want to use all predictors in both models.

```{r}
mod3 <- pscl::zeroinfl(y ~ trt, data = dat, dist = "poisson")
summary(mod3)
```

Notice trt is included in the Zero-inflation model, but it doesn't appear to be useful.

The `zeroinfl()` can also model negative binomial count models. Set `dist = "negbin`. We demonstrate this in the next Code Along.

See the Appendix to this notebook for simulating zero-inflated negative binomial data.


## CODE ALONG 4

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac). 

Recall the distribution of PhD articles. Might there be 0 inflation? Maybe some candidates are trying to publish but haven't just yet. And there may be others who have no intention to publish. Two sources of zeroes.

```{r}
plot(table(d$articles))
```

1. Fit a zero-inflated _negative binomial_ model using all predictors for both models. Call it "phd.zinb". Use `dist = "negbin`



2. Compare the zero-inflated negative binomial model to the negative binomial model we fit earlier (phd.nb) using AIC and BIC.



3. Compare rootograms for the two models.



## Effect plots 

Effect plots help us visualize our models. They allow us to see the effects of certain predictors holding others at a common value such as a mean, median or mode. We essentially make predictions with our model using various values of a "focal predictor" with all other predictors set to a fixed value.

Effect plots are very useful for visualizing _interactions_ and _non-linear effects_.

### Interaction example

Below we model count of articles published as a function of prestige, mentor, and their interaction. This says the effect of prestige depends on mentor and vice versa. What's the effect of prestige? It depends on mentor.

```{r}
phd.nb2 <- glm.nb(articles ~ prestige + mentor +
                   prestige:mentor, data = d)
summary(phd.nb2)
```

The interaction coefficient appears to be small and negative. What does it mean? How do these two variable interact? Let's create an effect plot.


```{r}
mentor_eff <- ggpredict(model = phd.nb2, terms = c("mentor", "prestige"))
plot(mentor_eff)

```

Let's fix prestige to values of 1, 3, and 5

```{r}
mentor_eff <- ggpredict(model = phd.nb2, 
                        terms = c("mentor", "prestige[1,3,5]"))
plot(mentor_eff)

```

Let's put prestige on the x-axis and color lines by mentor at values of 3, 6, 9, and 12

```{r}
mentor_eff <- ggpredict(model = phd.nb2, 
                        terms = c("prestige","mentor[3,6,9,12]"))
plot(mentor_eff)
```

This interaction, while statistically significant, seems very mild and not practically significant.

### Non-linear effect example

A common approach to modeling non-linear effects is to use a flexible cubic spline. Below we model the count of articles as function of mentor with a 3 degree of freedom natural cubic spline. The 3 degrees of freedom says we think the trajectory of the count could potentially change 3 times as mentor increases. One way to think of splines is as a more sophisticated version of polynomials (eg, x + x^2 + x^3).

Below we load the splines package that comes with R and use the `ns()` function to create what is called a "spline basis". This basically transforms the single mentor variable into three separate variables to allow for a non-linear relationship. Notice the output is impossible to interpret other than to conclude that the non-linear effects may be justified.

```{r}
library(splines)
phd.nb3 <- glm.nb(articles ~ ns(mentor, df = 3), data = d)
summary(phd.nb3)
```

Once again effect plots can help us visualize the non-linear effect.

```{r}
mentor_eff <- ggpredict(model = phd.nb3, terms = "mentor")
plot(mentor_eff)

```

We can look at just a certain range of mentor using the syntax `mentor[10:40]`

```{r}
mentor_eff <- ggpredict(model = phd.nb3, terms = "mentor[10:40]")
plot(mentor_eff)
```

There seems to be leveling off in the expected count of articles published by a student as the number articles published by the student's mentor increases. 

## CODE ALONG 5

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac). 

1.) Does the effect of prestige on the count of articles depend on whether or not the student is married? Model articles as a function of prestige, married, and their interaction. Name the model "phd.nb4"


2) Create an effect plot to visualize the interaction. Place prestige on the x-axis.




## Wrap-up

This was meant to show you the basics of count modeling in R. Hopefully you have a better grasp of how count modeling works. Scroll down for a few more topics.

What we did today works for _independent counts_. Things get more complicated when we have _multiple measures_ on the same subject or _clusters_ of related subjects or observations. In that case we usually use mixed-effect or longitudinal count models. 

Two packages to be aware of are lme4 and glmmTMB.

- Multilevel/mixed-effect poisson models: `lme4::glmer()`
- Multilevel/mixed-effect negative-binomial models: `lme4::glmer.nb()`
- Multilevel/mixed-effect zero-inflated models: `glmmTMB::glmmTMB()`

It's also not unheard of to just use linear models for counts. Sometimes they work just as well and can be easier to explain.

## References

- Builder C and Loughin T(2015). Analysis of Categorical Data with R. CRC Press. URL: http://www.chrisbilder.com/categorical/

- Friendly M and Meyer D. (2016). Discrete Data Analysis with R. CRC Press. URL: http://ddar.datavis.ca/

- Gelman A and Hill J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge

- Kleiber C, Zeileis A (2016). “Visualizing Count Data Regressions Using Rootograms.” The American Statistician, 70(3), 296–303. doi:10.1080/00031305.2016.1173590.  

- Long, J. S. (1997). Regression Models for Categorical and Limited Dependent Variables, Sage.

- Zeileis A, Kleiber C, and Jackman S. (2008). “Regression Models for Count Data in R.” Journal of Statistical Software, 27(8). URL http://www.jstatsoft.org/v27/i08/. 


## Appendix: Rate models 

Not all counts are created equal. For example two intersections may have similar counts of accidents per month. But if one intersection handles much less traffic than the other and has a similar number of accidents, then clearly it seems more dangerous.

To analyze data like this we need to take into account the amount of traffic, or opportunities for accidents. This is often called the _exposure_.

Let's simulate some data to illustrate.

```{r}
set.seed(4) 

# Exposure = number of opportunities for something to happen
exposure <- round(runif(n = 200, min = 100, max = 20000))

# some sort of grouping or treatment condition
trt <- sample(0:1, 200, replace = TRUE)

# generate counts where the mean is exp(3) if trt == 0 and exp(5) if trt == 1.
count <- rpois(n = 200, lambda = exp(3 + 2*trt))

# rate is count/exposure 
rate_data <- data.frame(count, trt, exposure, 
                        rate = count/exposure)
head(rate_data)

```

Notice the counts for observations 2 and 5 are similar (129 vs 124), but the exposures, and thus the rates, are very different (0.464 vs 0.007).
 
To model this data appropriately we should _include the exposure_. To do this we log transform the "exposure" and use either the offset argument or include in the model formula using the `offset()` function

```{r}
rate.mod1 <- glm(count ~ trt, offset = log(exposure), 
                 data = rate_data, family = poisson)
# same thing
rate.mod2 <- glm(count ~ trt + offset(log(exposure)), 
                 data = rate_data, family = poisson)

# check that coefficients are equal
all.equal(rate.mod1$coefficients, rate.mod2$coefficients)
```


When we exponentiate the coefficients we get the estimated _rate_ rather than the expected count.

```{r}
summary(rate.mod1)
```

Estimated _rate_ when when trt = 0.

```{r}
exp(coef(rate.mod1)["(Intercept)"])
```

The estimated rate when trt = 1 is exp(-6.19872 + 1.92684)

```{r}
exp(sum(coef(rate.mod1)))
```

The estimated rate when trt = 1 is exp(1.927) = 6.9 times that of the rate when trt = 0.

```{r}
exp(coef(rate.mod1)["trt"])
```

To use the model to make predictions, we need to include an exposure. Below we get predicted values for the two levels of trt with exposure set to 10,000. (Notice we don't need to log transform the exposure when using `predict()`.) The default prediction type is "link". In this case the link is the log transform. Notice the result closely matches the "true" values we used to simulate the data. 

```{r}
predict(rate.mod1, type = "link",
        newdata = data.frame(trt = c(0,1), exposure = c(10000, 10000)))
```

To get the predicted values as _counts_, we need to specify the type as "response". Below we get _predicted counts per 10,000_.

```{r}
predict(rate.mod1, type = "response",
        newdata = data.frame(trt = c(0,1), exposure = c(10000, 10000)))
```

To get the predicted values as _rates_, we need to specify the type as
"response" and divide by the exposure.

```{r}
p <- predict(rate.mod1, type = "response",
        newdata = data.frame(trt = c(0,1), exposure = c(10000, 10000)))
data.frame(p, rate = p/10000)

```

It doesn't matter what exposure you pick, the rate will be the same:

```{r}
p <- predict(rate.mod1, type = "response",
             newdata = data.frame(trt = c(0,1), exposure = c(12345, 12345)))
data.frame(p, rate = p/12345)

```

Notice the rates match what we got above by exponentiating the coefficients.

We can create effect plots using the ggeffects package, but there are some modifications we need to make to plot rates:

- need to use ggeffect() instead of ggpredict().
- ggeffects expects the model to have been fit with the offset argument
 specified (as opposed to being included in the model formula)
- We need to add rates to the data frame created by ggeffect()

The following plots the expected counts per 10,000, not the incidence rates

```{r}
eff_out <- ggeffect(rate.mod1, terms = "trt", offset = log(10000))
plot(eff_out)

```


Notice the fit is a line that treats trt as if it's a continuous number. Let's refit the model with trt as a factor.
 
```{r}
rate_data$trtF <- factor(rate_data$trt)
rate.mod <- glm(count ~ trtF, offset = log(exposure), 
                 data = rate_data, family = poisson)
eff_out <- ggeffect(rate.mod, terms = "trtF", offset = log(10000))
plot(eff_out)

```
 

To plot expected rates, we need to divide the fit and CI lower and upper bounds by 10,000. 


```{r}
vars <- c("predicted", "conf.low", "conf.high")
eff_out[vars] <- eff_out[vars]/1000
```


Once we do that, we have to create the plot ourselves:

```{r}
library(ggplot2)
ggplot(eff_out, aes(x, predicted)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.05)

```

Instead of per 10,000, we could do per 1,000, but notice the plot does not change:

```{r}
eff_out2 <- ggeffect(rate.mod, terms = "trtF", offset = log(1000))
eff_out2[vars] <- eff_out2[vars]/1000
ggplot(eff_out2, aes(x, predicted)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.05)

```

Changing the exposure _modifies expected counts but not expected rates_.

## Appendix: Hurdle models

Another type of model for excess zeroes is the hurdle model. In this case we assume there is one and only one process for the zeroes, and a separate process for counts greater than 0.

Example: number of cigarettes smoked in one week. 

- Zero counts are non-smokers
- non-zero counts are smokers (ie, zero truncated)

We can simulate data as before using the `rztpois()` function from the countreg package, which simulates data from a zero-truncated poisson distribution.

two means: 0.9 when trt = 0; 2.4 when trt = 1

```{r}
trt <- rep(0:1, each = 300)
mu <- 0.9 + 1.5*trt 
set.seed(20)
zi <- rbinom(600, size = 1, prob = 0.4) # ~60% zeroes
```

Now simulate counts conditional on our draws from the binomial distribution. If it's a 0, keep it. Otherwise take the result from a zero-truncated Poisson distribution with mean = exp(0.9 + 1.5*trt). NOTE: we could also use the negative binomial distribution

```{r}
y <- ifelse(zi == 0, 0, rztpois(600, mean=exp(mu)))  
dat <- data.frame(y, trt)
plot(table(dat$y))
```

Fit a model to our data that does not accommodate excess zeroes

```{r}
mod1 <- glm(y ~ trt, data = dat, family = poisson)
summary(mod1)
countreg::rootogram(mod1)
```


Fit a model that accommodates excess zeroes using the `hurdle()` function from the pscl package. This fits two models, separated by the |

1. a zero-truncated Poisson count model: y ~ trt
2. a binomial model: 1 (intercept only model)

```{r}
mod2 <- pscl::hurdle(y ~ trt | 1, data = dat, dist = "poisson")
summary(mod2)
```


```{r}
countreg::rootogram(mod2)
```

Use the `plogis()` function to take the inverse logit of the binomial model coefficient to get the estimated probability of clearing the "hurdle" (ie, having a count greater than 0)

```{r}
plogis(coef(mod2)["zero_(Intercept)"])
```

In this case, this is simply the proportion of counts NOT 0

```{r}
mean(dat$y != 0)
```

The binomial part of the hurdle model is the "Hurdle" to clear before the count model process kicks in. In this simple model, we might say we estimate about 40% of the subjects to clear the hurdle, which is what we specified when we generated the data.

## Appendix: creating rootograms "by hand" 

Recall this model and associate rootogram.

```{r}
phd.pois <- glm(articles ~ female + mentor, 
                data = d, 
                family = poisson)
rootogram(phd.pois)
```


Here's how we can make the rootogram "by hand"

Get expected mean count at each observation. The model returns the predicted mean count at each observation.

```{r}
mu <- predict(phd.pois, type = "response")
```

Next we create an empty matrix. Using the expected mean count for each observation, we calculate the probability of a 0 count, a 1 count, 2 count, etc all the way to a 9 count. The `dpois()` function returns the expected probability of a count for a given mean (lambda).

```{r}
p <- matrix(NA, nrow = length(mu), ncol = length(0:9))
for (i in 0:9) p[, i + 1L] <- dpois(i, lambda = mu)
```


Then we sum the columns (the probabilities) to get an expected count of 0, 1,...9. We also take the square root to put all the expected counts on a similar scale.

```{r}
expctd <- sqrt(colSums(p))
expctd
```


Now get the observed counts and take the square root

```{r}
obs <- sqrt(as.vector(table(d$articles)[1:10]))
```

Put everything into a data frame including the difference between expected and observed counts. This is our data frame to create the rootogram.

```{r}
d <- data.frame(count = 0:9,
                obs,
                expctd,
                diff = expctd - obs)
```


Make the rootogram. It's a little different than the one produced by `rootogram()` but it works.

```{r}
ggplot(d) +
  geom_segment(aes(x = count, y = diff, 
                   xend = count, yend = expctd),
               linewidth = 6, alpha = 1/4) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_point(aes(x = count, y = expctd), col = "red") +
  geom_line(aes(x = count, y = expctd), col = "red") +
  scale_x_continuous(breaks = 0:9, minor_breaks = NULL)

```

## Appendix: glm residuals 

How response, pearson and deviance residuals are calculated.

```{r}
# obs 1
head(d[,c("articles", "female", "mentor")], n = 1)
```

obs 1 is a male (female=0) and has 7 mentors, so they're predicted value is calculated as follows.

```{r}
beta <- coef(phd.pois)
mu_hat <- exp(beta[1] + beta[2]*0 + beta[3]*7)
obs <- d$articles[1]
```

Response residual for obs 1

```{r}
residuals(phd.pois, type = "response")[1]
```

```{r}
obs - mu_hat
```

pearson residual for obs 1

```{r}
residuals(phd.pois, type = "pearson")[1]
```

```{r}
(obs - mu_hat)/sqrt(mu_hat)

```

deviance residual for obs 1

```{r}
residuals(phd.pois, type = "deviance")[1]
```

```{r}
sign(obs - mu_hat) * sqrt(2*(obs * log(mu_hat) - (obs - mu_hat)))

```


## Appendix: Fitting quasipoisson models

Another approach to handling overdispersion is to use quasi-likelihood models. This estimates a _GLM dispersion parameter_ that is used to increase the standard errors of the coefficients. 

Below we fit two models:  

1. Poisson model
2. Quasipoisson model

In the second model we set `family = quasipoisson(link = "log")`. This results in a GLM dispersion parameter being estimated. Notice the model coefficients are the same as those estimated from a Poisson model.

```{r}
# regular Poisson model
phd.pois <- glm(articles ~ female + mentor, 
                data = d, 
                family = poisson)
# Quasi-Poisson model
phd.qpois <- glm(articles ~ female + mentor, 
                data = d, 
                family = quasipoisson(link = "log"))
# coefficients are the same
cbind("poisson" = coef(phd.pois), "quasipoisson" = coef(phd.qpois))
```

However the standard errors are different. The quasipoisson model has bigger standard errors. They've been inflated by the dispersion parameter.

```{r}
cbind("poisson" = coef(summary(phd.pois))[,"Std. Error"],
      "quasi-poisson" = coef(summary(phd.qpois))[,"Std. Error"])
```

Notice the sentence in the summary output of the quasipoisson model: "(Dispersion parameter for quasipoisson family taken to be 1.851488)"

```{r}
summary(phd.qpois)
```

That's the value used to adjust the standard errors. The standard errors from the regular poisson model are multiplied by this value.

The dispersion parameter is 1.851488. If we multiply the variance of the regular Poisson coefficients by this value and take the square root, we get the quasi-poisson standard errors.

```{r}
sqrt(coef(summary(phd.pois))[,"Std. Error"]^2 * 1.851488)
```

Notice this matches the quasi-poisson standard errors

```{r}
coef(summary(phd.qpois))[,"Std. Error"]
```

The dispersion parameter is estimated by taking the sum of the squared Pearson residuals and dividing by the residual degrees of freedom. (there's a bit of rounding error here when we do it by hand)

```{r}
sum(residuals(phd.pois, type = "pearson")^2)/phd.pois$df.residual
```

Notice also the summary output for the quasipoisson model does not return an AIC estimate. That's because have a quasi-likelihood, not a true likelihood. This is one reason why negative binomial models are usually preferred to model overdispersion in count models. 

## Appendix: simulate zero-inflated negative binomial data

We can also simulate zero-inflated data that includes a negative binomial count model. 

Once again we start by simulating zeroes and ones using `rbinom()` which allows us to randomly sample from a binomial distribution. This time we use a sample size of 1000.

```{r}
set.seed(6)
zi <- rbinom(1000, size = 1, prob = 0.7)
```

Next we simulate counts _conditional_ on our draws from the binomial distribution. If it's a 0, keep it. These are the "always zero" values. Otherwise take the result from a negative binomial distribution with mean = exp(0.6 + 1.8*trt) and theta = 9. This will produce additional zeroes.

The mean of the negative binomial distribution is as follows: 

1. 0.6 when trt = 0
2. 0.6 + 1.8 = 24 when trt = 1

```{r}
set.seed(66)
trt <- rep(0:1, each = 500)
y2 <- ifelse(zi == 0, 0, rnbinom(1000, mu=exp(0.6 + 1.8*trt), size = 9))  
dat2 <- data.frame(y2, trt)
plot(table(dat2$y2))
```

If we model the data without accounting for the excess zeroes we get a badly fitting rootogram.

```{r}
mod3 <- glm.nb(y2 ~ trt, data = dat2)
countreg::rootogram(mod3)
```

It's worth repeating: underfitting zeroes and overfitting counts of 1 and 2 is characteristic of zero-inflated data.

Now let's fit a model that accommodates excess zeroes using the `zeroinfl()` function. This fits two models, separated by the |

1. a negative binomial count model: y2 ~ trt
2. a binomial model: 1 (intercept only model)

The summary shows the results for two models: a count model and a zero-inflation model.

```{r}
mod4 <- pscl::zeroinfl(y2 ~ trt | 1, data = dat2, dist = "negbin")
summary(mod4)
```

The rootogram looks great. It should. We're fitting the "correct" model.

```{r}
countreg::rootogram(mod4)
```

And if we take the inverse logit of the binomial model, we see an estimated probability that is close to the "true" value of 0.3.

```{r}
plogis(coef(mod4)["zero_(Intercept)"])

```
