---
title: "Modeling Count Data with R"
author: "Clay Ford, Statistical Research Consultant, UVA Library"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

## Quick Intro to R Notebooks and R Markdown

This is an R Markdown Notebook. When you execute R code within the notebook, the results appear beneath the code.  

This file was created in RStudio by going to File...New File...R Notebook.

R code needs to be in "chunks" in an R Markdown Notebook. Below is an example of an R code chunk. It makes a parabola.

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter* (Win/Linux) or *Cmd+Shift+Return* (Mac). 

```{r}
x <- seq(-1, 1, by = 0.01)
y <- x^2
plot(x, y, type = "l")
```

To hide the output, click the Expand/Collapse output button. To clear results (or an error), click the "x". 

You can also press *Ctrl+Enter* (Win/Linux) or *Cmd+Return* (Mac) to run one line of code at a time (instead of the entire chunk).

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).  

## CODE ALONG 0

Insert a new R code chunk below and type and run the code: Sys.time()


## Packages

Let's load some packages we're going to use today.

```{r}
# install.packages("countreg", repos="http://R-Forge.R-project.org")
library(countreg)

library(pscl)
library(AER)
library(ggplot2)
library(ggeffects)
```

## Linear Model Review

Let's say we have the following data:

```{r}
x <- 1:25
y <- 10 + 5*x
plot(x, y)
```


10 is the intercept, 5 is the slope.
y is completely determined by x.

Let's add some "noise" to our data by adding random draws from a Normal distribution with mean = 0 and a standard deviation = 10.

set.seed(1) ensures we all get the same "random" data

```{r}
set.seed(1)
noise <- rnorm(n = 25, mean = 0, sd = 10)
```


Add the noise to 10 + 5*x and re-draw plot

```{r}
y <- 10 + 5*x + noise
plot(x, y)
```


This data is the combination of two parts:

1. 10 + 5*x
2. rnorm(n = 25, mean = 0, sd = 10)

We could also combine the two parts in a single call to `rnorm()` where we enter `10 + 5*x` as the mean. In other words, randomly generate Normal data with a _conditional mean_.

```{r}
set.seed(1)
y <- rnorm(n = 25, mean = 10 + 5*x, sd = 10)
plot(x, y)
```


Each y is drawn from a Normal distribution with mean _dependent on x_.

What if we were given this data and told to determine the process that
generated it? In other words, work backwards and fill in the blanks:

1. ____ + ____*x  (fixed part)
2. rnorm(n = 25, mean = 0, sd = ____)  (random part)

That's basically what linear modeling is. Proposing a model and finding the most likely values of the model parameters.

Traditional linear modeling assumes the following:

1. the formula is a weighted sum of predictors (eg, 10 + 5*x)
2. the noise is a random draw from a Normal distribution with mean = 0
3. the standard deviation of the Normal distribution is constant

Linear modeling tries to recover the weights (or coefficients) in the first
assumption (10 and 5) and the standard deviation in the 3rd assumption (10).

To do this we can use lm(). The syntax "y ~ x" means we think Part 1 of the model is "y = intercept + slope*x". We usually save the model into an object. Below I save it to "mod".

```{r}
mod <- lm(y ~ x)
summary(mod)
```

The model returns the following estimates:

- intercept = 11.135
- slope = 5.042
- sd = 9.7 (Residual standard error)

We can use our model to generate data and see if it looks similar to our original data.

```{r}
y_sim <- simulate(mod, nsim = 20)
plot(density(y))
for(i in 1:20)lines(density(y_sim[[i]]), col = "grey80")

```

Hence this is linear modeling:

1. propose and fit model(s)
2. determine if the model is good
3. use the model to explain relationships or make predictions

Today we look at _count models_ where we relax the assumptions that each response value is drawn from a Normal distribution with constant variance.

## Distributions for counts

### Poisson distribution

Use the `rpois()` function to generate random data from a Poisson distribution with a specified mean. Notice the data are positive integers greater than or equal to 0.

lambda is the mean; the variance and mean are equal in Poisson distributions.

Sample data from a Poisson distribution with a mean of 3. Note that lambda can take decimal values.

```{r}
set.seed(1)
y1 <- rpois(n = 1000, lambda = 3)
# counts of the distinct values
table(y1)

```


visual of the distribution of counts

```{r}
table(y1) |> plot()
```


We can also visualize the proportion of counts.

```{r}
table(y1) |> proportions() |> plot()
```

Notice the mean and variance are roughly equal:

```{r}
c(mean(y1), var(y1)) 
```



### Negative binomial distribution

We can use the `rnbinom()` function to generate random data from a negative binomial distribution with a specified mean and dispersion parameter (theta).

mu is the mean. The size argument is the dispersion parameter (theta). The negative binomial allows the _variance to be greater than the mean_.

Var = mu + mu^2/theta

Let's sample data from a negative binomial distribution with a mean = 3 and theta = 2. Notice the data are positive integers greater than or equal to 0.

```{r}
set.seed(2)
y2 <- rnbinom(n = 1000, mu = 3, size = 2)
# counts of the distinct values
table(y2)

```

Visualize the distribution of counts

```{r}
table(y2) |> plot()
```

Visualize the proportion of counts:

```{r}
table(y2) |> proportions() |> plot()
```

The mean and variance are not equal. The variance is larger

```{r}
c(mean(y2), var(y2)) 
```

As theta gets big, the negative binomial converges to a Poisson distribution. Notice in the randomly generated data below with a huge theta that the estimated mean and variance are about equal, like a Poisson distribution.

```{r}
y3 <- rnbinom(n = 1000, mu = 3, size = 2000)
c(mean(y3), var(y3)) 
```



## Simulating count data with conditional mean

Above we plugged in a single mean. But perhaps the mean depends on some other variable(s). Perhaps the mean is lower for an "untreated" group than the mean for a "treated" group.

Let's simulate a bunch of 0s and 1s to indicate untreated and treated

```{r}
set.seed(11)
trt <- sample(0:1, size = 1000, replace = T)
```

Now we randomly sample from a Poisson distribution with a mean of `exp(1.2 + 1.8*trt)`

```{r}
set.seed(22)
y1 <- rpois(n = 1000, lambda = exp(1.2 + 1.8*trt))
```


Why use exp()? That ensures we get a positive mean. Lambda has to be positive. Here it's not necessary but as we'll see this transformation is built into count models.

expected count when trt = 0

```{r}
exp(1.2) # about 3
```

expected count when trt = 1

```{r}
exp(1.2 + 1.8) # about 20
```

Plot of y1 distribution. Notice the two distinct humps for each group.

```{r}
table(y1) %>% plot()
```

Let's do the same for a negative binomial distribution

Randomly sample from a negative binomial distribution with a mean of `exp(1.2 + 1.8*trt)` and a dispersion of 10. Notice again the two distinct humps and that the data is much more dispersed (ie, has more variance)

```{r}
set.seed(33)
y2 <- rnbinom(n = 1000, mu = exp(1.2 + 1.8*trt), size = 10)
table(y2) %>% plot()
```

## Count modeling with simulated data

Let's pretend we don't know the formula used to generate the counts: 
1.2 + 1.8*trt. 

How to recover those "true" values? This is essentially count modeling.

### Fitting a Poisson count model

We use the `glm()` function to fit a poisson count model. Use the same way as `lm()`, but include the family argument: `family = poisson` 

The `link = "log"` argument is assumed by default but is included here for completeness.

Let's fit a model to our simulated data. We generated the data so we know the correct model to specify!

Th "correct" model:
`y1 <- rpois(n = 1000, lambda = exp(1.2 + 1.8*trt))`

`y1 ~ trt` is shorthand for "y1 = (Intercept) + slope*trt"

```{r}
m1 <- glm(y1 ~ trt, family = poisson(link = "log"))
summary(m1)
```

We can extract the coefficients using `coef()`

```{r}
coef(m1)
```

The coefficients are very close to the "true" values of 1.2 and 1.8.

Notice the coefficients are on the log scale. We need to use `exp()` to get the coefficients on the original scale. The original scale is the _expected counts_.

When trt == 0 the expected mean count is about 3.2.

exp(1.176377 + 1.814068 * 0) = exp(1.176377)

```{r}
coef(m1)["(Intercept)"] |> exp()
```

When trt == 1 (sum both coefficients) the expected mean count is about 20

exp(1.176377 + 1.814068 * 1) = exp(2.990444)

```{r}
sum(coef(m1)) |> exp()
```

Exponentiating the slope coefficient tells us the _multiplicative effect_ of trt == 1. Expected count of y when trt = 1 is about 6.1 times the expected count when trt = 0.

```{r}
coef(m1)["trt"] |> exp()
```

Again, the expected count of y when trt = 1 is about 6.1 times the expected count when trt = 0.

```{r}
exp(coef(m1)["(Intercept)"]) * 6.109849
```

This is precisely the expected count we calculated above.

```{r}
sum(coef(m1)) %>% exp()

```

Let's use our model to simulate some data and see if it looks similar to our observed data set.

```{r}
sim_y1 <- rpois(n = 1000, lambda = exp(1.176377 + 1.814068*trt))
par(mfrow = c(1,2))
plot(table(y1), main = "original")
plot(table(sim_y1), main = "model simulated")

```


We could also plot smooth density curves that approximate the distribution, though some may argue this is not appropriate for discrete data. 

```{r}
sim_y1 <- simulate(m1, nsim = 50)
plot(density(y1, from = 0))
for(i in 1:50)lines(density(sim_y1[[i]], from = 0), 
                    col = "grey80", lty = 3)
```

### Fitting a Negative binomial count model

We use the `glm.nb()` function from the MASS package to fit a negative binomial model. No need to specify the family argument. The `link = log` argument is assumed by default but is included here for completeness.

Let's fit a model to our simulated data. We generated the data so we know the correct model to specify!

`y2 <- rnbinom(n = 1000, mu = exp(1.2 + 1.8*trt), size = 10)`

```{r}
m2 <- glm.nb(y2 ~ trt, link = log)
summary(m2)
```

We can extract theta from the model object if needed:

```{r}
m2$theta
```

And we can use `coef()` to extract the coefficients.

```{r}
coef(m2)
```

Again, the coefficients are very close to the "true" values of 1.2, 1.8 and 1.2

And again we need to use `exp()` to get our expected counts on the original scale, which is expected counts.

When trt == 0 the expected mean count is about 3.3

```{r}
coef(m2)["(Intercept)"] |> exp()
```

When trt == 1 (sum both coefficients) the expected mean count is about 19.6

```{r}
sum(coef(m2)) |> exp()
```

The multiplicative effect of trt==1 is about 5.9.

```{r}
coef(m2)["trt"] |> exp()
```

Let's use our model to simulate some data and see if it looks similar to our oberved data set.

```{r}
sim_y2 <- rnbinom(n = 1000, mu = exp(coef(m2)[1] + coef(m2)[2]*trt), 
                  size = m2$theta)
par(mfrow = c(1,2))
plot(table(y2), main = "original")
plot(table(sim_y2), main = "model simulated")

```

We could also plot smooth density curves that approximate the distribution

```{r}
sim_y2 <- simulate(m2, nsim = 50)
plot(density(y2, from = 0))
for(i in 1:50)lines(density(sim_y2[[i]], from = 0), 
                    col = "grey80", lty = 3)
```

## CODE ALONG 1

Run the following lines of code to generate y

```{r}
set.seed(4)
grp <- sample(c("a","b"), size = 300, replace = T)
x1 <- runif(n = 300, min = 1, max = 10)
y <- rpois(n = 300, lambda = exp(1.8 + 1.5*(grp=="b") + -0.6*x1))
```

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).  

1. Fit a model that attempts to "recover" the true values in the model specified above.


2. Simulate data from the model and compare to the original observed data.

## Deviance


Let's look again at the summary of m1, the poisson model:

```{r}
summary(m1)
```


Notice the lines for Null deviance and Residual deviance. 

```
    Null deviance: 7784.1  on 999  degrees of freedom
Residual deviance: 1099.3  on 998  degrees of freedom

```

Residuals represent the difference between what we observed and what our model predicts. There are several types of residuals for GLM models. The Null and Residual deviance are the sum of squares of the _deviance residuals_.

Residual deviance "by hand"

```{r}
sum(residuals(m1, type = "deviance")^2)
```


Null deviance "by hand"

The null deviance is the sum of squared deviance residuals for a model with no predictors, just an intercept: y1 ~ 1

```{r}
null_m1 <- glm(y1 ~ 1, family = poisson)
sum(residuals(null_m1, type = "deviance")^2)

```

Three things to remember about deviance...

1. Deviance is a measure of error, lower means better
2. If a predictor is just noise and has no explanatory power, we expect deviance to decrease by 1 on average.
3. When a useful predictor is added to a model, we expect deviance to decrease by more than 1.

We see a huge decrease in deviance in m1. 

## Poisson count modeling with real data

Let's load some real data to explore and model. A data set giving the number of publications (articles) by doctoral candidates in biochemistry in relation to various predictors. 

Source: "Discrete Data Analysis with R" by Michael Friendly and David Meyer.

Of interest is if the count of articles can be modeled as a function of the other variables. We don't know the "true" model or whether or not these variables have anything to do with the count of articles.

```{r}
data("PhdPubs", package = "vcdExtra")
str(PhdPubs)
```

- articles: number of articles published in final three years of PhD studies
- female: indicator variable for gender, 1 = female
- married: indicator variable for marital status, 1 = married
- kid5: number of young children, age 5 and under
- phdprestige: prestige of the PhD department
- mentor: number of publications by the mentor in the preceding three years

```{r}
summary(PhdPubs)
```

Make female and married Factors

```{r}
PhdPubs$female <- factor(PhdPubs$female, labels = c("no", "yes"))
PhdPubs$married <- factor(PhdPubs$married, labels = c("no", "yes"))
```

Plot the response variable: number of published articles

```{r}
plot(table(PhdPubs$articles))
```

The number of published articles has quite a bit of variability. Do the other variables perhaps "explain" some of the variability?

Let's fit a Poisson model using female and mentor. 

```{r}
phd.pois <- glm(articles ~ female + mentor, 
                data = PhdPubs, 
                family = poisson)
summary(phd.pois)
```

Naive interpretation of coefficients.

The coefficients represent the increase/decrease in the _log expected count_ of articles. 

```{r}
coef(phd.pois)
```


If we exponentiate, we get the multiplicative effect (holding other predictors constant.)

```{r}
exp(coef(phd.pois))

```


If we exponentiate, multiply by 100 and subtract 1, we get the percent increase or decrease.

```{r}
100 * (exp(coef(phd.pois)) - 1)

```


All together in a matrix using cbind(), rounded to 3 decimal places

```{r}
round(
  cbind(beta = coef(phd.pois),
        exp_beta = exp(coef(phd.pois)),
        pct = 100 * (exp(coef(phd.pois)) - 1)),
  3)

```

The expected number of publications for female candidates is about 0.83 times that of male candidates, or about a 17% decrease

Each additional mentor publication increases expected count of published articles by a factor of about 1.025, or about 2.5%.

Using both predictors seem to be better than using just an intercept according to the null and residual deviance.


### Confidence intervals

Confidence intervals give us a sense of how certain these estimated effects are. Since we exponentiate, overlapping 1 (instead of 0) indicates an uncertain direction in magnitude.

```{r}
exp(confint(phd.pois))

```


The effect of female is estimated to decrease expected published article count anywhere from a factor of 0.75 to 0.92, or 25% to 8%.

We should check the fit of the model before putting too much stock into these interpretations. We could do something like the following, where we simulate counts from our model and compare to the observed data. Notice our model is estimating much fewer 0 counts and far fewer bigger counts. Re-run the chunk multiple times if you like.

```{r}
sim_articles <- simulate(phd.pois)
par(mfrow = c(1,2))
plot(table(PhdPubs$articles))
plot(table(sim_articles$sim_1))
```

This doesn't look good. We'll explore another model fit visualization below called _rootograms_.

## CODE ALONG 2

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac). 

1. Add kid5 to the previous model and save the result as `phd.pois2`:

```
phd.pois <- glm(articles ~ female + mentor, 
                data = PhdPubs, family = poisson)
```

```{r}
phd.pois2 <- glm(articles ~ female + mentor + kid5, 
                data = PhdPubs, family = poisson)

```


2. Interpret the coefficient for kid5


## Evaluating model fit with rootograms 

A good fitting model should generate counts that look similar to the original data.

In the previous code along exercise we proposed the following model:

`articles = (Intercept) + female + married + kid5`

The result of the model was an intercept and coefficients for the 3 predictors

```{r}
coef(phd.pois2)

```


Previously we simulated data and compared bar plots of the original data with the simulated data. Another way to visualize model fit for count models is through _rootograms_.

The countreg package provides the rootogram() function to quickly generate "hanging" rootograms to visualize goodness of fit.

```{r}
countreg::rootogram(phd.pois2)
```

The red dots and connector lines visualize the model's expected number of articles.

The length of the bars are the _observed counts_. Notice the counts have been transformed with a square root transformation. That's to help smaller counts not get squashed by larger counts.

Bars that dip below 0 are underfit; bars that hang above 0 are overfit.

We can use the max argument to increase or decrease the count displayed

```{r}
countreg::rootogram(phd.pois, max = 15)
```


There are two other styles available: standing and suspended

The standing version basically plots the model predicted counts over the oberserved counts.

```{r}
countreg::rootogram(phd.pois2, style = "standing")
```

The suspended version uses less ink and shows just the under and over-fit counts.

```{r}
countreg::rootogram(phd.pois2, style = "suspended")
```

ggplot2 version

```{r}
autoplot(rootogram(phd.pois2))
```

This is an ok not great fitting model. We are underfitting 0 counts, overfitting
1, 2, and 3 counts, then underfitting counts 4 - 7.

Recall our earlier toy model for which we fit the "correct" model. Notice we're not consistently over or underfitting counts. The counts seem to vary around 0 randomly.

```{r}
countreg::rootogram(m1)
```

See Appendix at conclusion of notebook for how to make rootograms "by hand".

## CODE ALONG 3

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac). 

1. Add married and phdprestige to the following model and save as a new model object named "phd.pois3".

```
phd.pois <- glm(articles ~ female + mentor + kid5, data = PhdPubs, family = poisson)
```

2. Create a rootogram of the model

## Negative Binomial count modeling with real data

Recall the negative binomial model accommodates unequal mean and variance. This allows us to model "overdispersion". That is, when we have counts with larger variance than their mean.

The PhdPubs data appears to exhibit some overdispersion

```{r}
c(mean(PhdPubs$articles), 
  var(PhdPubs$articles))
  
```


The AER package provides a formal test of overdispersion with the `dispersiontest()` function. The null is the mean and variance are equal. Rejecting the null with a small p-value provides evidence of overdispersion.

```{r}
phd.pois3 <- glm(articles ~ female + mentor + kid5 + married + phdprestige,
                 data = PhdPubs, family = poisson)
```

The first argument is a fitted Poisson model.

```{r}
AER::dispersiontest(phd.pois3)
```

The super small p-value suggests we probably have overdispersion and that a Poisson model may not be suitable, which is also what we saw with the rootogram. In this case a negative binomial model may be more suitable since it accommodates overdispersion.
 
The glm.nb function in the MASS package fits a negative binomial model

```{r}
phd.nb <- glm.nb(articles ~ ., data = PhdPubs)
summary(phd.nb)
```


The interpretation of the coefficients is the same as a Poisson model.

```{r}

exp(coef(phd.nb))

```
 
Is this model better than the Poisson model? We can asses this question using the AIC/BIC information criteria. Models with lower values are preferred.

```{r}
AIC(phd.pois3, phd.nb)
```

```{r}
BIC(phd.pois3, phd.nb)
```


A rootogram allows us to asses how well the model fits the observed data.

```{r}
countreg::rootogram(phd.nb)
```

This certainly seems to fit much better.

## Zero-inflated count models

It's not unusual for real life count data to have more zeroes than would be expected of a Poisson or negative binomial model. When this happens we say we have "zero inflation".

A common approach to modeling such data is to create a mixture of two models to account for the extra zeroes:

1. a model for the occurrence of zeroes
2. a model for the counts, which can also include zeroes

Example: survey random sample of UVA students and ask how many alcoholic drinks they consumed over the most recent weekend.

- Some don't drink, always zero
- Some do drink but perhaps didn't drink that weekend, second source of zeroes

Let's simulate data with these qualities.

To begin we'll simulate zeroes and ones using `rbinom()` which allows us to randomly sample from a binomial distribution. A binomial distribution has two parameters: 

- size (number of "trials") 
- probability (chance of "success"). 

Below we generate 300 draws from a binomial distribution with size = 1 and prob = 0.7. This says 70% of the time we will draw a 1, which implies 30% of the time we will draw 0.

```{r}
set.seed(10)
zi <- rbinom(300, size = 1, prob = 0.7)
```

Next we'll create a binary predictor variable called "trt"

```{r}
trt <- rep(0:1, each = 150)
```

Now we'll simulate counts _conditional_ on our draws from the binomial distribution. If it's a 0, keep it. These are the "always zero" values. Otherwise take the result from a Poisson distribution with mean = exp(0.05 + 0.8*trt). This will produce additional zeroes.

The mean of the Poisson distribution is as follows: 

1. 0.05 when trt = 0
2. 0.05 + 0.8 = 0.85 when trt = 1

```{r}
set.seed(11)
y <- ifelse(zi == 0, 0, rpois(300, lambda=exp(0.05 + 0.8*trt)))  
dat <- data.frame(y, trt)
plot(table(dat$y))
```

Now let's fit a model to our data that does _not_ accommodate excess zeroes and look at the rootogram

```{r}
mod1 <- glm(y ~ trt, data = dat, family = poisson)
summary(mod1)
```

The summary looks good if you're into significant p-values, but the rootogram tells a different story.

```{r}
countreg::rootogram(mod1)
```

Underfitting zeroes and overfitting counts of 1 and 2 is characteristic of zero-inflated data.
 
Now let's fit a model that accommodates excess zeroes using the `zeroinfl()` function from the pscl package. This fits two models, separated by the |

1. a Poisson count model: y ~ trt
2. a binomial model: 1 (intercept only model)

Notice the summary shows results for two models: a count model and a zero-inflation model.

```{r}
mod2 <- pscl::zeroinfl(y ~ trt | 1, data = dat, dist = "poisson")
summary(mod2)
```

The rootogram shows a much better fitting model.

```{r}
countreg::rootogram(mod2)
```


We can use the `plogis()` function to take the inverse logit of the binomial model coefficient to get the estimated probability of being an "always zero" count. Notice we come close to the "true" value of 0.3 we used to simulate the data.
 
```{r}
plogis(coef(mod2)["zero_(Intercept)"])
```


We can also simulate zero-inflated data that includes a negative binomial count model.

Once again we start by simulating zeroes and ones using `rbinom()` which allows us to randomly sample from a binomial distribution. This time we use a sample size of 1000.

```{r}
set.seed(6)
zi <- rbinom(1000, size = 1, prob = 0.7)
```

Next we simulate counts _conditional_ on our draws from the binomial distribution. If it's a 0, keep it. These are the "always zero" values. Otherwise take the result from a negative binomial distribution with mean = exp(0.6 + 1.8*trt) and theta = 9. This will produce additional zeroes.

The mean of the negative binomial distribution is as follows: 

1. 0.6 when trt = 0
2. 0.6 + 1.8 = 24 when trt = 1

```{r}
set.seed(66)
trt <- rep(0:1, each = 500)
y2 <- ifelse(zi == 0, 0, rnbinom(1000, mu=exp(0.6 + 1.8*trt), size = 9))  
dat2 <- data.frame(y2, trt)
plot(table(dat2$y2))
```

If we model the data without accounting for the excess zeroes we get a badly fitting rootogram.

```{r}
mod3 <- glm.nb(y2 ~ trt, data = dat2)
countreg::rootogram(mod3)
```

It's worth repeating: underfitting zeroes and overfitting counts of 1 and 2 is characteristic of zero-inflated data.

Now let's fit a model that accommodates excess zeroes using the `zeroinfl()` function. This fits two models, separated by the |

1. a negative binomial count model: y2 ~ trt
2. a binomial model: 1 (intercept only model)

The summary shows the results for two models: a count model and a zero-inflation model.

```{r}
mod4 <- pscl::zeroinfl(y2 ~ trt | 1, data = dat2, dist = "negbin")
summary(mod4)
```


The rootogram looks great. It should. We're fitting the "correct" model.

```{r}
countreg::rootogram(mod4)
```

And if we take the inverse logit of the binomial model, we see an estimated probability that is close to the "true" value of 0.3.

```{r}
plogis(coef(mod4)["zero_(Intercept)"])

```

If we don't specify anything after the |, the `zeroinfl()` function assumes you want to use all predictors in both models.

```{r}
mod5 <- pscl::zeroinfl(y2 ~ trt, data = dat2, dist = "negbin")
summary(mod5)
```

Notice trt is included in the Zero-inflation model, but it doesn't appear to be useful.

## CODE ALONG 4

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac). 

Recall the distribution of PhD articles. Might there be 0 inflation? Maybe some candidates are trying to publish but haven't just yet. And there may be others who have no intention to publish. Two sources of zeroes.

```{r}
plot(table(PhdPubs$articles))

```

1. Fit a zero-inflated negative binomial model using all predictors. Call it "phd.zinb". 

2. Compare the zero-inflated negative binomial model to the negative binomial model we fit earlier (phd.nb) using AIC and BIC.


3. Compare rootograms for the two models.


## Effect plots 

Effect plots help us visualize our models. They allow us to see the effects of certain predictors holding others at a common value such as a mean, median or mode. We essentially make predictions with our model using various values of a "focal predictor" with all other predictors set to a fixed value.

Effect plots are very useful for visualizing _interactions_ and _non-linear effects_.

### Interaction example

Below we model count of articles published as a function of phdprestige, mentor, and their interaction. This says the effect of phdprestige depends on mentor and vice versa. What's the effect of phdprestige? It depends on mentor.

```{r}
phd.nb2 <- glm.nb(articles ~ phdprestige + mentor +
                   phdprestige:mentor, data = PhdPubs)
summary(phd.nb2)
```

The interaction coefficient appears to be small and negative. What does it mean? How do these two variable interact? Let's create an effect plot.


```{r}
mentor_eff <- ggpredict(model = phd.nb2, terms = c("mentor", "phdprestige"))
plot(mentor_eff)

```


```{r}
mentor_eff <- ggpredict(model = phd.nb2, terms = c("mentor", "phdprestige[1,3,5]"))
plot(mentor_eff)

```

```{r}
mentor_eff <- ggpredict(model = phd.nb2, 
                        terms = c("phdprestige","mentor[3,6,9,12]"))
plot(mentor_eff)
```


### Non-linear effect example

A common approach to modeling non-linear effects is to use a flexible cubic spline. Below we model the count of articles as function of mentor with a 3 degree of freedom natual cubic spline. The 3 degrees of freedom says we think the trajectory of the count could potentially change 3 times as mentor increases. One way to think of splines is as a more sophisticated version of polynomials (eg, x + x^2 + x^3).

Below we load the splines package that comes with R and use the `ns()` function to create what is called a "spline basis". This basically transforms the single mentor variable into three separate variables to allow for a non-linear relationship. Notice the output is impossible to interpret other than to conclude that the non-linear effects may be justified.

```{r}
library(splines)
phd.nb3 <- glm.nb(articles ~ ns(mentor, df = 3), data = PhdPubs)
summary(phd.nb3)
```

Once again effect plots can help us visualize the non-linear effect.

```{r}
mentor_eff <- ggpredict(model = phd.nb3, terms = "mentor")
plot(mentor_eff)

```

We can look at just a certain range of mentor using the syntax `mentor[10:40]`

```{r}
mentor_eff <- ggpredict(model = phd.nb3, terms = "mentor[10:40]")
plot(mentor_eff)
```


## Rate models 

Not all counts are created equal. For example two intersections may have similar counts of accidents per month. But if one intersection handles much less traffic than the other and has a similar number of accidents, then clearly it seems more dangerous.

Let's simulate some data to illustrate.

```{r}
set.seed(4) 

# Exposure = number of opportunities for something to happen
exposure <- round(runif(n = 200, min = 100, max = 20000))

# some sort of grouping or treatment condition
trt <- sample(0:1, 200, replace = TRUE)

# generate counts where the mean is exp(3) if trt == 0 and exp(5) if trt == 1.
count <- rpois(n = 200, lambda = exp(3 + 2*trt))

# rate is count/exposure 
rate_data <- data.frame(count, trt, exposure, 
                        rate = count/exposure)
head(rate_data)

```


Notice the counts for observations 2 and 5 are similar, but the exposures, and thus the rates, are very different.
 
To model this data appropriately we should include the exposure. To do this we log transform the "exposure" and use either the offset argument or include in the model formula using the `offset` function

```{r}
rate.mod1 <- glm(count ~ trt, offset = log(exposure), 
                 data = rate_data, family = poisson)
# same thing
rate.mod2 <- glm(count ~ trt + offset(log(exposure)), 
                 data = rate_data, family = poisson)

# check that coefficients are equal
all.equal(rate.mod1$coefficients, rate.mod2$coefficients)
```


The interpretation is in terms of the _rate_ rather than the mean

```{r}
summary(rate.mod1)
```

Notice the intercept is nowhere close to the true value of 3. It is the estimated _rate_ when all other explanatory variables are set to 0. In this case that's the estimated rate when trt = 0.

```{r}
exp(coef(rate.mod1)["(Intercept)"])
```


The estimated rate when trt = 1 is exp(1.927) = 6.9 times that of the rate when trt = 0.

```{r}
exp(coef(rate.mod1)["trt"])
```

The estimated rate when trt = 1 is exp(-6.19872 + 1.92684)

```{r}
exp(sum(coef(rate.mod1)))
```

To use the model to make predictions, we need to include an exposure. Below we get predicted values for the two levels of trt with exposure set to 10,000. (Notice we don't need to log transform the exposure.) The default prediction type is "link". In this case the link is the log transform. Notice the result closely matches the "true" values we used to simulate the data. 

```{r}
predict(rate.mod1, type = "link",
        newdata = data.frame(trt = c(0,1), exposure = c(10000, 10000)))
```

To get the predicted values as _counts_, we need to specify the type as "response". Below we get _predicted counts per 10,000_.

```{r}
predict(rate.mod1, type = "response",
        newdata = data.frame(trt = c(0,1), exposure = c(10000, 10000)))
```

To get the predicted values as _rates_, we need to specify the type as
"response" and divide by the exposure.

```{r}
p <- predict(rate.mod1, type = "response",
        newdata = data.frame(trt = c(0,1), exposure = c(10000, 10000)))
data.frame(p, rate = p/10000)

```

It doesn't matter what exposure you pick, the rate will be the same:

```{r}
p <- predict(rate.mod1, type = "response",
             newdata = data.frame(trt = c(0,1), exposure = c(12345, 12345)))
data.frame(p, rate = p/12345)

```

Notice the rates match what we got above by exponentiating the coefficients.

We can create effect plots using the ggeffects package, but there are some modifications we need to make to plot rates:

- need to use ggeffect() instead of ggpredict().
- ggeffects expects the model to have been fit with the offset argument
 specified (as opposed to being included in the model formula)
- We need to add rates to the data frame created by ggeffect()

The following plots the expected counts per 10,000, not the incidence rates

```{r}
eff_out <- ggeffect(rate.mod1, terms = "trt", offset = log(10000))
plot(eff_out)

```


Notice the fit is a line that treats trt as if it's a continuous number. Let's refit the model with trt as a factor.
 
```{r}
rate_data$trtF <- factor(rate_data$trt)
rate.mod <- glm(count ~ trtF, offset = log(exposure), 
                 data = rate_data, family = poisson)
eff_out <- ggeffect(rate.mod, terms = "trtF", offset = log(10000))
plot(eff_out)

```
 

To plot expected rates, we need to divide the fit and CI lower and upper bounds by 10,000. We do this below with mutate_at


```{r}
vars <- c("predicted", "conf.low", "conf.high")
eff_out[vars] <- eff_out[vars]/1000
```


Once we do that, we have to create the plot ourselves:

```{r}
library(ggplot2)
ggplot(eff_out, aes(x, predicted)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.05)

```

Instead of per 10,000, we could do per 1,000, but notice the plot does not change:

```{r}
eff_out2 <- ggeffect(rate.mod, terms = "trtF", offset = log(1000))
eff_out2[vars] <- eff_out2[vars]/1000
ggplot(eff_out2, aes(x, predicted)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1)

```

Changing the exposure _modifies expected counts but not expected rates_.

## Wrap-up

This was meant to show you the basics of count modeling in R. Hopefully you have a better grasp of how count modeling works. Scroll down for a few more topics.

What we did today works for _independent counts_. Things get more complicated when we have _multiple measures_ on the same subject or _clusters_ of related subjects or observations. In that case we usually use mixed-effect or longitudinal count models. 

Two packages to be aware of are lme4 and glmmTMB.

- Multilevel/mixed-effect poisson models: `lmer4::glmer()`
- Multilevel/mixed-effect negative-binomial models: `lme4::glmer.nb()`
- Multilevel/mixed-effect zero-inflated models: `glmmTMB::glmmTMB()`

It's also not unheard of to just use linear models for counts. Sometimes they work just as well and can be easier to explain.

## Appendix: Hurdle models

Another type of model for excess zeroes is the hurdle model. In this case we assume there is one and only one process for the zeroes, and a separate process for counts greater than 0.

Example: number of cigarettes smoked in one week. 

- Zero counts are non-smokers
- non-zero counts are smokers (ie, zero truncated)

We can simulate data as before using the `rztpois()` function from the countreg package, which simulates data from a zero-truncated poisson distribution.

two means: 0.9 when trt = 0; 2.4 when trt = 1

```{r}
trt <- rep(0:1, each = 300)
mu <- 0.9 + 1.5*trt 
set.seed(20)
zi <- rbinom(600, size = 1, prob = 0.4) # ~60% zeroes
```

Now simulate counts conditional on our draws from the binomial distribution. If it's a 0, keep it. Otherwise take the result from a zero-truncated Poisson distribution with mean = exp(0.9 + 1.5*trt). NOTE: we could also use the negative binomial distribution

```{r}
y <- ifelse(zi == 0, 0, rztpois(600, mean=exp(mu)))  
dat <- data.frame(y, trt)
plot(table(dat$y))
```

Fit a model to our data that does not accommodate excess zeroes

```{r}
mod1 <- glm(y ~ trt, data = dat, family = poisson)
summary(mod1)
countreg::rootogram(mod1)
```


Fit a model that accommodates excess zeroes using the `hurdle()` function from the pscl package. This fits two models, separated by the |

1. a zero-truncated Poisson count model: y ~ trt
2. a binomial model: 1 (intercept only model)

```{r}
mod2 <- pscl::hurdle(y ~ trt | 1, data = dat, dist = "poisson")
summary(mod2)
```


```{r}
countreg::rootogram(mod2)
```

Use the `plogis()` function to take the inverse logit of the binomial model coefficient to get the estimated probability of clearing the "hurdle" (ie, having a count greater than 0)

```{r}
plogis(coef(mod2)["zero_(Intercept)"])
```

In this case, this is simply the proportion of counts NOT 0

```{r}
mean(dat$y != 0)
```

The binomial part of the hurdle model is the "Hurdle" to clear before the count model process kicks in. In this simple model, we might say we estimate about 40% of the subjects to clear the hurdle, which is what we specified when we generated the data.

## Appendix: rootograms "by hand" 

Recall this model and associate rootogram.

```{r}
phd.pois <- glm(articles ~ female + mentor, 
                data = PhdPubs, 
                family = poisson)
rootogram(phd.pois)
```


Here's how we can make the rootogram "by hand"

Get expected mean count at each observation. The model returns the predicted mean count at each observation.

```{r}
mu <- predict(phd.pois, type = "response")
```

Next we create an empty matrix. Using the expected mean count for each observation, we calculate the probability of a 0 count, a 1 count, 2 count, etc all the way to a 9 count. The `dpois()` function returns the expected probability of a count for a given mean (lambda).

```{r}
p <- matrix(NA, nrow = length(mu), ncol = length(0:9))
for (i in 0:9) p[, i + 1L] <- dpois(i, lambda = mu)
```


Then we sum the columns (the probabilities) to get an expected count of 0, 1,...9. We also take the square root to put all the expected counts on a similar scale.

```{r}
expctd <- sqrt(colSums(p))
expctd
```


Now get the observed counts and take the square root

```{r}
obs <- sqrt(as.vector(table(PhdPubs$articles)[1:10]))
```

Put everything into a data frame including the difference between expected and observed counts. This is our data frame to create the rootogram.

```{r}
d <- data.frame(count = 0:9,
                obs,
                expctd,
                diff = expctd - obs)
```


Make the rootogram. It's a little different than the one produced by `rootogram()` but it works.

```{r}
ggplot(d) +
  geom_segment(aes(x = count, y = diff, 
                   xend = count, yend = expctd),
               linewidth = 6, alpha = 1/4) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_point(aes(x = count, y = expctd), col = "red") +
  geom_line(aes(x = count, y = expctd), col = "red") +
  scale_x_continuous(breaks = 0:9, minor_breaks = NULL)

```

## Appendix: glm residuals 

How response, pearson and deviance residuals are calculated.

```{r}
# obs 1
head(PhdPubs[,c("articles", "female", "mentor")], n = 1)
```

obs 1 is a male (female=0) and has 7 mentors, so they're predicted value is calculated as follows.

```{r}
beta <- coef(phd.pois)
mu_hat <- exp(beta[1] + beta[2]*0 + beta[3]*7)
obs <- PhdPubs$articles[1]
```

Response residual for obs 1

```{r}
residuals(phd.pois, type = "response")[1]
```

```{r}
obs - mu_hat
```

pearson residual for obs 1

```{r}
residuals(phd.pois, type = "pearson")[1]
```

```{r}
(obs - mu_hat)/sqrt(mu_hat)

```

deviance residual for obs 1

```{r}
residuals(phd.pois, type = "deviance")[1]
```

```{r}
sign(obs - mu_hat) * sqrt(2*(obs * log(mu_hat) - (obs - mu_hat)))

```





